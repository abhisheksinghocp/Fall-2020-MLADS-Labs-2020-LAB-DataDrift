{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Machine Learning Model Data Drift using Azure ML & Azure Data Bricks \n",
    "# Session Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Change is the only constant in life‚Äù adage holds true for machine learning (ML) models in production, as over time they could deteriorate in their accuracy or in their predictive power as the distribution of input features change overtime typically, called data drift. This is more pronounced during times of extreme environment like COVID.\n",
    "\n",
    "The goal of this tutorial is to provide an overview of the data drift concept as well as a walk-through of detecting data drift for in-production ML models in a scalable & efficient way using Azure Databricks & Azure ML .\n",
    "\n",
    "# Description\n",
    "Data drift is one of the top reasons model accuracy degrades over time. For machine learning models, data drift is the change in model input data that leads to model performance degradation. Monitoring data drift helps detect these model performance issues apriori. Causes of data drift include:\n",
    "\n",
    "1. Upstream process changes like metric definition changes\n",
    "2. Data quality issues, such as a broken CRM system which shows Revenue as null\n",
    "3. Natural drift in the data, such as Revenue changing across quarters or due to external environment like COVID\n",
    "\n",
    "Our goal in this tutorial is to convey a theoretical and practical aspect of data drift used for in-production models in a scalable & efficient manner. The learnings can be applied for any machine learning model in any business.\n",
    "\n",
    "We will talk about the power of Azure Machine Learning which simplifies data drift detection by computing a single metric abstracting the complexity of datasets being compared. These datasets may have hundreds of features and tens of thousands of rows. Once drift is detected, you can drill down into which features are causing the drift. You then inspect feature level metrics to debug and isolate the root cause for the drift. This top down approach makes it easy to monitor data instead of traditional rules-based techniques. All this can be done for multiple models in an automated way leveraging the power of Azure Data Bricks and Azure ML.\n",
    "\n",
    "The fundamental idea this tutorial will aim to convey is that incorporating Data drift for machine learning model would help in detecting model performance degradation apriori and reduce the cost of prediction gone wrong in a scalable and automated way.\n",
    "\n",
    "# Prerequisites\n",
    "1. Azure subscription. If you don't have an Azure subscription, create a free account before you begin.\n",
    "2. Azure Machine Learning workspace. Try the free or paid version of Azure Machine Learning today.\n",
    "3. Azure Machine Learning SDK for Python installed, which includes the azureml-datasets package.\n",
    "4. Azure Data Bricks with runtime 6.0 and above and Azure Data Lake storage(ADLS) mounted in it.\n",
    "5. Training Data and Scoring Data in Parquet format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.installPyPI(\"azureml-datadrift\", version =\"1.10.0\")\n",
    "dbutils.library.installPyPI(\"azureml-sdk\", version =\"1.10.0\")\n",
    "dbutils.library.installPyPI(\"pyarrow\", version =\"0.17.1\")\n",
    "dbutils.library.installPyPI(\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3a-Create a Data drift monitor for each individual model\n",
    "# Step 3b-Upload training and scoring data files in Azure ML DataStore for individual model for each run\n",
    "# Step 3c- Set up backfill time window for each individual model\n",
    "    We have created a python function which takes care of all three steps and can be also be used for scaling for multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import pandas as pd\n",
    "from azureml.core import *\n",
    "from azureml.core import Workspace\n",
    "print('SDK version:', azureml.core.VERSION)\n",
    "from azureml.core import Workspace, Datastore\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.datadrift import DataDriftDetector, AlertConfiguration\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "prinid = dbutils.secrets.get(scope='<>' , key ='<>')\n",
    "prinpass = dbutils.secrets.get(scope='<>' , key ='<>')\n",
    "\n",
    "sp = ServicePrincipalAuthentication(tenant_id = \"<>\",\n",
    "                                    service_principal_id = prinid,\n",
    "                                    service_principal_password=prinpass)\n",
    "\n",
    "ws = Workspace.get(name=\"<>\",\n",
    "               auth=sp,    \n",
    "               subscription_id='<>',\n",
    "               resource_group='<>'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Function for creating data drift functionality for each model and running it on a daily basis\n",
    "def data_drift_function(model_files_path, model_datastore_name, model_datamonitor_name,code_type,exclude_list=None,alert_email_Address=None):\n",
    "  ################ v1 Ankit Gupta(Please reach ****@microsoft.com in case of questions) ########################\n",
    "  ################ Function to enable drift monitor, upload files in data store and run a backfill job\n",
    "  ################ model_files_path       = ADLS path where model files are uploaded. For Eg-/dbfs/mnt/data/ModelOutput/Data_Drift/ACR_Account_Forecasting_Model\n",
    "  ################ model_datastore_name   = Model data store folder name . For Eg-'ACR_Propensity_Longterm'\n",
    "  ################ exclude_list =  list of features need to exclude from data drift monitor. Eg-['OpportunityID]\n",
    "  ################ code_type = Code used for creating data drift raw files.EG- Python or R\n",
    "  ################ alert_email_Address    = email to recieve alerts from the scheduled pipeline after enabling. Eg-['****@microsoft.com'] \n",
    "  ################ model_datamonitor_name = Model Data monitor name . Eg- ACR_prop_long_datadrift\n",
    "    \n",
    "  dstore = ws.get_default_datastore()############getting default datastore linked to subcription which will be used for uploading data\n",
    "  dstore.upload(model_files_path, model_datastore_name, overwrite=True, show_progress=True) #############uploading latest model files into datastrore\n",
    "  \n",
    "  if str(code_type).upper().strip() == 'PYTHON':\n",
    "    baseline = Dataset.Tabular.from_parquet_files(dstore.path(model_datastore_name + '/Training/**' )) ##################assigning baseline for Data drift\n",
    "    #baseline = baseline.register(ws, model_datastore_name + '_baseline')\n",
    "\n",
    "\n",
    "    target = Dataset.Tabular.from_parquet_files(dstore.path(model_datastore_name + '/Scoring/**')) ##############assigning target for Data drift\n",
    "    target = target.with_timestamp_columns('scoring_date') ###############Assigning Date column for data drift target\n",
    "    #target = target.register(ws, model_datastore_name + '_target') ####### register the target dataset\n",
    "  else:\n",
    "    #print('else running')\n",
    "    baseline = Dataset.Tabular.from_parquet_files(dstore.path(model_datastore_name + '/Training/*.snappy.parquet' )) ##################assigning baseline for Data drift\n",
    "    #baseline = baseline.register(ws, model_datastore_name + '_baseline')\n",
    "\n",
    "\n",
    "    target = Dataset.Tabular.from_parquet_files(dstore.path(model_datastore_name + '/Scoring/*/*.snappy.parquet')) ##############assigning target for Data drift\n",
    "    target = target.with_timestamp_columns('scoring_date') ###############Assigning Date column for data drift target\n",
    "    #target = target.register(ws, model_datastore_name + '_target') ####### register the target dataset\n",
    "    \n",
    "  #######################compute details section##################\n",
    "  compute_name = 'cluster'\n",
    "  \n",
    "  if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "  else:\n",
    "      print('creating a new compute target...')\n",
    "      provisioning_config = AmlCompute.provisioning_configuration(vm_size='cluster', min_nodes=0, max_nodes=4)\n",
    "\n",
    "      # create the cluster\n",
    "      compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "      # can poll for a minimum number of nodes and for a specific timeout.\n",
    "      # if no min node count is provided it will use the scale settings for the cluster\n",
    "      compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "      # For a more detailed view of current AmlCompute status, use get_status()\n",
    "      print(compute_target.get_status().serialize())\n",
    "      \n",
    "  alert_config = AlertConfiguration(alert_email_Address) ########assigning email address for alert. Currenlty not working as app insights features is not working\n",
    "  \n",
    "  try:\n",
    "    if DataDriftDetector.get_by_name(ws, model_datamonitor_name) :\n",
    "      print(model_datamonitor_name,'monitor found')\n",
    "      # get data drift detector by name\n",
    "      monitor = DataDriftDetector.get_by_name(ws, model_datamonitor_name)\n",
    "  except:\n",
    "    monitor = DataDriftDetector.create_from_datasets(ws, model_datamonitor_name, baseline, target, \n",
    "                                                              compute_target='cluster',         # compute target for scheduled pipeline and backfills \n",
    "                                                              frequency='Day',                     # how often to analyze target data\n",
    "                                                              feature_list=None,                    # list of features to detect drift on\n",
    "                                                              drift_threshold=None,                 # threshold from 0 to 1 for email alerting\n",
    "                                                              latency=0,                            # SLA in hours for target data to arrive in the dataset\n",
    "                                                              alert_config=None)            # email addresses to send alert\n",
    "  \n",
    "  \n",
    "  monitor = DataDriftDetector.get_by_name(ws, model_datamonitor_name)\n",
    "  \n",
    "  # create feature list - need to exclude columns that naturally drift or increment over time, such as year, day, index\n",
    "  columns  = list(baseline.take(1).to_pandas_dataframe())\n",
    "  exclude  = exclude_list  #,'__index_level_0__'\n",
    "  print(exclude)\n",
    "  print(exclude_list)\n",
    "  #print(model_files_path, model_datastore_name, model_datamonitor_name,code_type,exclude_list)\n",
    "  if exclude is not None:\n",
    "    features = [col for col in columns if col not in exclude]\n",
    "    print(features)\n",
    "    # update the feature list\n",
    "    monitor  = monitor.update(feature_list=features)\n",
    "  \n",
    "  monitor.disable_schedule()\n",
    "  monitor.enable_schedule()\n",
    "\n",
    "  target_df=target.to_pandas_dataframe(on_error='null', out_of_range_datetime='null')\n",
    "  target_df['scoring_date'] = target_df['scoring_date'].dt.strftime('%Y-%m-%d')\n",
    "  target_df['scoring_date'] = pd.to_datetime(target_df['scoring_date'],infer_datetime_format=True)\n",
    "  backfill_start_date = target_df['scoring_date'].min()\n",
    "  backfill_end_date = target_df['scoring_date'].max()\n",
    "  backfill = monitor.backfill(backfill_start_date, backfill_end_date)\n",
    "  monitor.enable_schedule()\n",
    "  \n",
    "  print ('################## Summary of the run ###############################')\n",
    "  print ('####### model_files_path       ',model_files_path)\n",
    "  print ('####### model_datastore_name   ',model_datastore_name)\n",
    "  print ('####### model_datamonitor_name ',model_datamonitor_name)\n",
    "  print ('####### exclude_list           ',exclude_list)\n",
    "  print ('####### code_type              ',code_type)\n",
    "  print ('####### alert_email_Address    ',alert_email_Address)\n",
    "  print ('####### backfill               ',backfill)\n",
    "  print ('######################################################################')\n",
    "  return backfill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Create a file with metadata information for all the models\n",
    "Metadata file format = Id, datastore_monitor_name, model_files_path, alert_email_Address, ignore_list\n",
    "We have used a csv file with above metadata information for example\n",
    "\n",
    "# Step 5 Run the data drift monitor for multiple model in sequence\n",
    "Using below python code along with information stored in metadata file and data drift function, we can create/update/daily run the data drift for multiple model in sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_read = pd.read_csv('<MetaData file input>')\n",
    "\n",
    "output_list = []\n",
    "for i in range(len(metadata_read)):\n",
    "  print ('################## Input for the current run ###############################')\n",
    "  print ('####### model_files_path       ',metadata_read['model_files_path'][i])\n",
    "  print ('####### model_datastore_name   ',metadata_read['datastore_monitor_name'][i] )\n",
    "  print ('####### model_datamonitor_name ',metadata_read['datastore_monitor_name'][i] )\n",
    "  print ('####### exclude_list           ',metadata_read['ignore_list'][i] )\n",
    "  print ('####### code_type              ',metadata_read['code_type'][i] )\n",
    "  print ('####### alert_email_Address    ',metadata_read['alert_email_Address'][i] )\n",
    "  print ('######################################################################')\n",
    "  \n",
    "  model_files_path, model_datastore_name, model_datamonitor_name,code_type = metadata_read['model_files_path'][i],metadata_read['datastore_monitor_name'][i],metadata_read['datastore_monitor_name'][i],metadata_read['code_type'][i]\n",
    "  exclude_list = list(str(metadata_read['ignore_list'][i]).split(\",\"))\n",
    "  print('final parameter  ', model_files_path, model_datastore_name, model_datamonitor_name,code_type,exclude_list)\n",
    "  try:\n",
    "  #data_drift_function(model_files_path, model_datastore_name, model_datamonitor_name,exclude_list,code_type,['****@microsoft.com'])\n",
    "    backfill = data_drift_function(model_files_path, model_datastore_name, model_datamonitor_name,code_type,exclude_list,['****@microsoft.com'])\n",
    "    output_list.append(str(metadata_read['datastore_monitor_name'][i]) + ' ' + str(backfill))\n",
    "  except:\n",
    "    print('Error found for  ', model_files_path, model_datastore_name, model_datamonitor_name,exclude_list,code_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No of the moniter got created as part of current processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Model Process ',pd.DataFrame(output_list).count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "name": "data_drift_creation_function_final",
  "notebookId": 3561752747726436
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
