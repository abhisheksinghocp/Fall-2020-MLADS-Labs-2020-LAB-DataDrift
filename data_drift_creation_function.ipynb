{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.library.installPyPI(\"azureml-datadrift\", version =\"1.10.0\")\n",
    "dbutils.library.installPyPI(\"azureml-sdk\", version =\"1.10.0\")\n",
    "dbutils.library.installPyPI(\"pyarrow\", version =\"0.17.1\")\n",
    "dbutils.library.installPyPI(\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import pandas as pd\n",
    "from azureml.core import *\n",
    "from azureml.core import Workspace\n",
    "print('SDK version:', azureml.core.VERSION)\n",
    "from azureml.core import Workspace, Datastore\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.datadrift import DataDriftDetector, AlertConfiguration\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "prinid = dbutils.secrets.get(scope='<>' , key ='<>')\n",
    "prinpass = dbutils.secrets.get(scope='<>' , key ='<>')\n",
    "\n",
    "sp = ServicePrincipalAuthentication(tenant_id = \"<>\",\n",
    "                                    service_principal_id = prinid,\n",
    "                                    service_principal_password=prinpass)\n",
    "\n",
    "ws = Workspace.get(name=\"<>\",\n",
    "               auth=sp,    \n",
    "               subscription_id='<>',\n",
    "               resource_group='<>'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Function for creating data drift functionality for each model and running it on a daily basis\n",
    "def data_drift_function(model_files_path, model_datastore_name, model_datamonitor_name,code_type,exclude_list=None,alert_email_Address=None):\n",
    "  ################ v1 Ankit Gupta(Please reach ****@microsoft.com in case of questions) ########################\n",
    "  ################ Function to enable drift monitor, upload files in data store and run a backfill job\n",
    "  ################ model_files_path       = ADLS path where model files are uploaded. For Eg-/dbfs/mnt/data/ModelOutput/Data_Drift/ACR_Account_Forecasting_Model\n",
    "  ################ model_datastore_name   = Model data store folder name . For Eg-'ACR_Propensity_Longterm'\n",
    "  ################ exclude_list =  list of features need to exclude from data drift monitor. Eg-['OpportunityID]\n",
    "  ################ code_type = Code used for creating data drift raw files.EG- Python or R\n",
    "  ################ alert_email_Address    = email to recieve alerts from the scheduled pipeline after enabling. Eg-['****@microsoft.com'] \n",
    "  ################ model_datamonitor_name = Model Data monitor name . Eg- ACR_prop_long_datadrift\n",
    "    \n",
    "  dstore = ws.get_default_datastore()############getting default datastore linked to subcription which will be used for uploading data\n",
    "  dstore.upload(model_files_path, model_datastore_name, overwrite=True, show_progress=True) #############uploading latest model files into datastrore\n",
    "  \n",
    "  if str(code_type).upper().strip() == 'PYTHON':\n",
    "    baseline = Dataset.Tabular.from_parquet_files(dstore.path(model_datastore_name + '/Training/**' )) ##################assigning baseline for Data drift\n",
    "    #baseline = baseline.register(ws, model_datastore_name + '_baseline')\n",
    "\n",
    "\n",
    "    target = Dataset.Tabular.from_parquet_files(dstore.path(model_datastore_name + '/Scoring/**')) ##############assigning target for Data drift\n",
    "    target = target.with_timestamp_columns('scoring_date') ###############Assigning Date column for data drift target\n",
    "    #target = target.register(ws, model_datastore_name + '_target') ####### register the target dataset\n",
    "  else:\n",
    "    #print('else running')\n",
    "    baseline = Dataset.Tabular.from_parquet_files(dstore.path(model_datastore_name + '/Training/*.snappy.parquet' )) ##################assigning baseline for Data drift\n",
    "    #baseline = baseline.register(ws, model_datastore_name + '_baseline')\n",
    "\n",
    "\n",
    "    target = Dataset.Tabular.from_parquet_files(dstore.path(model_datastore_name + '/Scoring/*/*.snappy.parquet')) ##############assigning target for Data drift\n",
    "    target = target.with_timestamp_columns('scoring_date') ###############Assigning Date column for data drift target\n",
    "    #target = target.register(ws, model_datastore_name + '_target') ####### register the target dataset\n",
    "    \n",
    "  #######################compute details section##################\n",
    "  compute_name = 'cluster'\n",
    "  \n",
    "  if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "  else:\n",
    "      print('creating a new compute target...')\n",
    "      provisioning_config = AmlCompute.provisioning_configuration(vm_size='cluster', min_nodes=0, max_nodes=4)\n",
    "\n",
    "      # create the cluster\n",
    "      compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "      # can poll for a minimum number of nodes and for a specific timeout.\n",
    "      # if no min node count is provided it will use the scale settings for the cluster\n",
    "      compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "      # For a more detailed view of current AmlCompute status, use get_status()\n",
    "      print(compute_target.get_status().serialize())\n",
    "      \n",
    "  alert_config = AlertConfiguration(alert_email_Address) ########assigning email address for alert. Currenlty not working as app insights features is not working\n",
    "  \n",
    "  try:\n",
    "    if DataDriftDetector.get_by_name(ws, model_datamonitor_name) :\n",
    "      print(model_datamonitor_name,'monitor found')\n",
    "      # get data drift detector by name\n",
    "      monitor = DataDriftDetector.get_by_name(ws, model_datamonitor_name)\n",
    "  except:\n",
    "    monitor = DataDriftDetector.create_from_datasets(ws, model_datamonitor_name, baseline, target, \n",
    "                                                              compute_target='cluster',         # compute target for scheduled pipeline and backfills \n",
    "                                                              frequency='Day',                     # how often to analyze target data\n",
    "                                                              feature_list=None,                    # list of features to detect drift on\n",
    "                                                              drift_threshold=None,                 # threshold from 0 to 1 for email alerting\n",
    "                                                              latency=0,                            # SLA in hours for target data to arrive in the dataset\n",
    "                                                              alert_config=None)            # email addresses to send alert\n",
    "  \n",
    "  \n",
    "  monitor = DataDriftDetector.get_by_name(ws, model_datamonitor_name)\n",
    "  \n",
    "  # create feature list - need to exclude columns that naturally drift or increment over time, such as year, day, index\n",
    "  columns  = list(baseline.take(1).to_pandas_dataframe())\n",
    "  exclude  = exclude_list  #,'__index_level_0__'\n",
    "  print(exclude)\n",
    "  print(exclude_list)\n",
    "  #print(model_files_path, model_datastore_name, model_datamonitor_name,code_type,exclude_list)\n",
    "  if exclude is not None:\n",
    "    features = [col for col in columns if col not in exclude]\n",
    "    print(features)\n",
    "    # update the feature list\n",
    "    monitor  = monitor.update(feature_list=features)\n",
    "  \n",
    "  monitor.disable_schedule()\n",
    "  monitor.enable_schedule()\n",
    "\n",
    "  target_df=target.to_pandas_dataframe(on_error='null', out_of_range_datetime='null')\n",
    "  target_df['scoring_date'] = target_df['scoring_date'].dt.strftime('%Y-%m-%d')\n",
    "  target_df['scoring_date'] = pd.to_datetime(target_df['scoring_date'],infer_datetime_format=True)\n",
    "  backfill_start_date = target_df['scoring_date'].min()\n",
    "  backfill_end_date = target_df['scoring_date'].max()\n",
    "  backfill = monitor.backfill(backfill_start_date, backfill_end_date)\n",
    "  monitor.enable_schedule()\n",
    "  \n",
    "  print ('################## Summary of the run ###############################')\n",
    "  print ('####### model_files_path       ',model_files_path)\n",
    "  print ('####### model_datastore_name   ',model_datastore_name)\n",
    "  print ('####### model_datamonitor_name ',model_datamonitor_name)\n",
    "  print ('####### exclude_list           ',exclude_list)\n",
    "  print ('####### code_type              ',code_type)\n",
    "  print ('####### alert_email_Address    ',alert_email_Address)\n",
    "  print ('####### backfill               ',backfill)\n",
    "  print ('######################################################################')\n",
    "  return backfill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the input CSV with below format \n",
    "    <MetaData file input>\n",
    "    Id, datastore_monitor_name, model_files_path, training_date, code_type, alert_email_Address, ignore_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_read = pd.read_csv('<MetaData file input>')\n",
    "\n",
    "output_list = []\n",
    "for i in range(len(metadata_read)):\n",
    "  print ('################## Input for the current run ###############################')\n",
    "  print ('####### model_files_path       ',metadata_read['model_files_path'][i])\n",
    "  print ('####### model_datastore_name   ',metadata_read['datastore_monitor_name'][i] )\n",
    "  print ('####### model_datamonitor_name ',metadata_read['datastore_monitor_name'][i] )\n",
    "  print ('####### exclude_list           ',metadata_read['ignore_list'][i] )\n",
    "  print ('####### code_type              ',metadata_read['code_type'][i] )\n",
    "  print ('####### alert_email_Address    ',metadata_read['alert_email_Address'][i] )\n",
    "  print ('######################################################################')\n",
    "  \n",
    "  model_files_path, model_datastore_name, model_datamonitor_name,code_type = metadata_read['model_files_path'][i],metadata_read['datastore_monitor_name'][i],metadata_read['datastore_monitor_name'][i],metadata_read['code_type'][i]\n",
    "  exclude_list = list(str(metadata_read['ignore_list'][i]).split(\",\"))\n",
    "  print('final parameter  ', model_files_path, model_datastore_name, model_datamonitor_name,code_type,exclude_list)\n",
    "  try:\n",
    "  #data_drift_function(model_files_path, model_datastore_name, model_datamonitor_name,exclude_list,code_type,['****@microsoft.com'])\n",
    "    backfill = data_drift_function(model_files_path, model_datastore_name, model_datamonitor_name,code_type,exclude_list,['****@microsoft.com'])\n",
    "    output_list.append(str(metadata_read['datastore_monitor_name'][i]) + ' ' + str(backfill))\n",
    "  except:\n",
    "    print('Error found for  ', model_files_path, model_datastore_name, model_datamonitor_name,exclude_list,code_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No of the moniter got created as part of current processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Model Process ',pd.DataFrame(output_list).count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "name": "data_drift_creation_function_final",
  "notebookId": 3561752747726436
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
